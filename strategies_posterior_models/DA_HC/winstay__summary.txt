 Family: bernoulli 
  Links: mu = logit 
Formula: subject_left ~ 1 + stay_bias + leave_bias + (1 + stay_bias + leave_bias | subject) 
   Data: data (Number of observations: 2307) 
  Draws: 4 chains, each with iter = 1000; warmup = 0; thin = 1;
         total post-warmup draws = 4000

Group-Level Effects: 
~subject (Number of levels: 40) 
                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)                 0.06      0.05     0.00     0.18 1.00     1561     1320
sd(stay_bias)                 0.58      0.11     0.38     0.81 1.00     2047     2750
sd(leave_bias)                0.71      0.12     0.50     0.96 1.00     2246     2178
cor(Intercept,stay_bias)      0.13      0.39    -0.65     0.80 1.01      248      469
cor(Intercept,leave_bias)    -0.03      0.39    -0.73     0.74 1.01      231      511
cor(stay_bias,leave_bias)    -0.54      0.17    -0.83    -0.17 1.00     1376     2190

Population-Level Effects: 
           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept     -0.06      0.05    -0.16     0.04 1.00     4160     2850
stay_bias      0.84      0.12     0.61     1.09 1.00     1823     2581
leave_bias    -0.94      0.14    -1.21    -0.69 1.00     1825     2460

Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).
