 Family: bernoulli 
  Links: mu = logit 
Formula: subject_left ~ 1 + stay_bias + leave_bias + (1 + stay_bias + leave_bias | subject) 
   Data: data (Number of observations: 2275) 
  Draws: 4 chains, each with iter = 1000; warmup = 0; thin = 1;
         total post-warmup draws = 4000

Group-Level Effects: 
~subject (Number of levels: 40) 
                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)                 0.27      0.09     0.09     0.46 1.00      907      879
sd(stay_bias)                 0.96      0.15     0.71     1.29 1.00     1858     2869
sd(leave_bias)                1.01      0.14     0.76     1.33 1.00     1867     2865
cor(Intercept,stay_bias)     -0.23      0.24    -0.67     0.29 1.00      401      649
cor(Intercept,leave_bias)     0.35      0.22    -0.13     0.75 1.01      386      523
cor(stay_bias,leave_bias)     0.05      0.17    -0.29     0.38 1.00     1306     1942

Population-Level Effects: 
           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept     -0.05      0.07    -0.18     0.08 1.00     2418     3038
stay_bias      0.94      0.17     0.61     1.27 1.00     1442     2152
leave_bias    -0.78      0.17    -1.13    -0.45 1.00     1417     2105

Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).
